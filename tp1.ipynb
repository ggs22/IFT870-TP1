{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# IFT870 - TP1 \n",
    "### gibg2501 - leba3207\n",
    "#### 7 février 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# some imports...\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from IPython.display import Markdown as md\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import scipy.spatial.distance as sdist\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Magical word to inline figures in jupyter notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "# Disable annoyin warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# removes maximum number of columns & rows for display\n",
    "pd.options.display.max_columns = 7\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "# Determine how many outputs per cells should be displayed\n",
    "InteractiveShell.ast_node_interactivity = \"last\" #    'all', 'last', 'last_expr', 'none', 'last_expr_or_assign'\n",
    "\n",
    "# Loads the data in a data frame\n",
    "tp1_data_file = 'TP1_data.csv'\n",
    "tp1_data = pd.read_csv(tp1_data_file, header=0, index_col=0)\n",
    "tp1_data\n",
    "headers = tp1_data.columns\n",
    "\n",
    "# Create data frames for each classe\n",
    "condition0 = tp1_data['classe'] == 0\n",
    "condition1 = tp1_data['classe'] == 1\n",
    "condition2 = tp1_data['classe'] == 2\n",
    "\n",
    "tp1_data_classe_0 = tp1_data.loc[condition0 , 'attribut1':'attribut4']\n",
    "tp1_data_classe_1 = tp1_data.loc[condition1 , 'attribut1':'attribut4']\n",
    "tp1_data_classe_2 = tp1_data.loc[condition2 , 'attribut1':'attribut4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Description des données statistiques de base du jeux de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tp1_data.loc[:, 'attribut1':'attribut4'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.pairplot(tp1_data, vars=['attribut1', 'attribut2', 'attribut3', 'attribut4'],  hue='classe', kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des attributs du jeux de données paire à paire et quantification de la correlation à l'aide de la méthode de Pearson $$\\left(\\frac{cov(X,Y)}{\\sigma_X\\sigma_Y} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrélation inter-attributs pour la classe 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tp1_data_classe_0.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrélation inter-attributs pour la classe 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tp1_data_classe_1.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrélation inter-attributs pour la classe 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tp1_data_classe_2.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sns.regplot(tp1_data.loc[:,'attribut1'],tp1_data.loc[:,'attribut3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 & Q2\n",
    "a) Les relations de corrélations sont visiblement différentes pour les 3 classes.\n",
    "Par exemple, pour la classe 0 le 1er et le 3e, ainsi que le 2e et 3e attributs sont fortement et négativement corrélés.\n",
    "Les attributs 3 et 4 sont encore négativement corrélés, mais beausoup plus faiblement. Ces corrélations ne tiennent cependant plus pour la classe 1 et 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Utilisation de la méthode de réduction en composantes principales ($$\\textit{PCA}$$) pour quantifier les corrélations entre les attributs pour chaque classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Scale the features data and applies PCA tranform for all classes\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "features = tp1_data.loc[: , 'attribut1':'attribut4']\n",
    "labels = tp1_data.loc[: , 'classe']\n",
    "scaler.fit(features)\n",
    "tp1_data_scaled = scaler.transform(features)\n",
    "\n",
    "# PCA transform on scaled data\n",
    "PCA_scaled___data_tp1 = PCA(n_components = features.shape[1])\n",
    "PCA_scaled___data_tp1.fit(tp1_data_scaled, labels)\n",
    "transf___scaled_all = PCA_scaled___data_tp1.transform(tp1_data_scaled)\n",
    "\n",
    "# PCA transform on centered data\n",
    "PCA_centered_data_tp1 = PCA(n_components = features.shape[1])\n",
    "# Center features around 0\n",
    "# centered_features = features - np.mean(features)\n",
    "centered_features = features\n",
    "PCA_centered_data_tp1.fit(features, labels)\n",
    "transf_centered_all = PCA_centered_data_tp1.transform(features)\n",
    "\n",
    "# Create data frames from transformed features\n",
    "transf___scaled_all_labeled = pd.DataFrame(transf___scaled_all, columns = ['attribut1', 'attribut2', 'attribut3', 'attribut4'])\n",
    "transf___scaled_all_labeled.insert(4, 'classe', labels.array)\n",
    "\n",
    "transf_centered_all_labeled = pd.DataFrame(transf_centered_all, columns = ['attribut1', 'attribut2', 'attribut3', 'attribut4'])\n",
    "transf_centered_all_labeled.insert(4, 'classe', labels.array)\n",
    "\n",
    "# Create data frame from PCs\n",
    "data_tp1_components = pd.DataFrame(PCA_centered_data_tp1.components_, columns=list(features.columns))\n",
    "tp1_components_ratios = PCA_centered_data_tp1.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les ratios d\\'influence pour chaque composante principale: "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tp1_components_ratios"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On peut constater que les composante 1 et 1 sont conjointement responsable d\\'envrion 92% de la variance des données. On peut donc discarter les composantes 3 et 4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dipslay transformed features in 2D\n",
    "n_features = features.shape[1]\n",
    "fig, axes = plt.subplots(n_features - 1, n_features, figsize=(10,10))\n",
    "k = 0\n",
    "for i in range(n_features):\n",
    "    for j in range(n_features):\n",
    "        if(i<j):\n",
    "            axes[i,j].scatter(transf_centered_all[:, i], transf_centered_all[:, j], c = labels)\n",
    "\n",
    "        # axes[i,j].y_label['']\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate ranges and create data frames for each class\n",
    "lim_inf = 0\n",
    "lim_1 = tp1_data_classe_0.shape[0]+1\n",
    "lim_2 = lim_1 + tp1_data_classe_1.shape[0]\n",
    "lim_3 = lim_2 + tp1_data_classe_2.shape[0]\n",
    "\n",
    "transf_C0 = pd.DataFrame(transf_centered_all[lim_inf:lim_1, :], columns = headers[0:4]) \n",
    "transf_C1 = pd.DataFrame(transf_centered_all[lim_1-1:lim_2, :], columns = headers[0:4])\n",
    "transf_C2 = pd.DataFrame(transf_centered_all[lim_2-1:, :], columns = headers[0:4])\n",
    "\n",
    "# Process clustering for each class\n",
    "centroids_C0 = transf_C0.mean(axis=0)[0:3]\n",
    "centroids_C1 = transf_C1.mean(axis=0)[0:3]\n",
    "centroids_C2 = transf_C2.mean(axis=0)[0:3]\n",
    "\n",
    "centroids = np.array([centroids_C0, centroids_C1, centroids_C2])\n",
    "\n",
    "# Plot transformed features data according to the 2 best PC\n",
    "plt.scatter(transf_centered_all[:, 0], transf_centered_all[:, 1], c = labels)\n",
    "\n",
    "# Plots centroids\n",
    "plt.scatter(centroids_C0[0], centroids_C0[1], marker='x', c='r')\n",
    "plt.scatter(centroids_C1[0], centroids_C1[1], marker='x', c='r')\n",
    "plt.scatter(centroids_C2[0], centroids_C2[1], marker='x', c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def GetEuclidianDistancesErrors(transformed_values_labelled, centroids_arr, n_pc):\n",
    "    _errors_vect = np.array([])\n",
    "    _err_count = 0\n",
    "\n",
    "    for index, transf_point in transformed_values_labelled.iterrows():\n",
    "\n",
    "        if n_pc == 2:\n",
    "            _dist_c0 = sdist.euclidean([transf_point[0], transf_point[1]],\n",
    "                                       [centroids_arr [0, 0], centroids_arr [0, 1]])\n",
    "\n",
    "            _dist_c1 = sdist.euclidean([transf_point[0], transf_point[1]],\n",
    "                                        [centroids_arr [1, 0], centroids_arr [1, 1]])\n",
    "\n",
    "            _dist_c2 = sdist.euclidean([transf_point[0], transf_point[1]],\n",
    "                                        [centroids_arr [2, 0], centroids_arr [2, 1]])\n",
    "        elif n_pc == 3:\n",
    "            _dist_c0 = sdist.euclidean([transf_point[0], transf_point[1], transf_point[2]],\n",
    "                                        [centroids_arr [0, 0], centroids_arr [0, 1], centroids_arr [0, 2]])\n",
    "\n",
    "            _dist_c1 = sdist.euclidean([transf_point[0], transf_point[1], transf_point[2]],\n",
    "                                        [centroids_arr [1, 0], centroids_arr [1, 1], centroids_arr [1, 2]])\n",
    "\n",
    "            _dist_c2 = sdist.euclidean([transf_point[0], transf_point[1], transf_point[2]],\n",
    "                                        [centroids_arr [2, 0], centroids_arr [2, 1], centroids_arr [2, 2]])\n",
    "\n",
    "        distances = np.array([_dist_c0, _dist_c1, _dist_c2])\n",
    "\n",
    "        calculated_classe = np.where(distances == np.amin(distances))[0][0]\n",
    "        actual_classe = transf_point.loc['classe']\n",
    "\n",
    "        if calculated_classe != actual_classe:\n",
    "            _err = np.array([index, calculated_classe, actual_classe])\n",
    "            _errors_vect = np.concatenate((_errors_vect, _err))\n",
    "            _err_count += 1\n",
    "\n",
    "    _errors_vect = _errors_vect.reshape(_err_count, 3)\n",
    "    _errors_frame = pd.DataFrame(_errors_vect, columns=['index', 'calculated class', 'acutal class'])\n",
    "    return _errors_frame\n",
    "\n",
    "def GetManhattanDistancesErrors(transformed_values_labelled, centroids_arr, n_pc):\n",
    "    _errors_vect = np.array([])\n",
    "    _err_count = 0\n",
    "\n",
    "    for index, transf_point in transformed_values_labelled.iterrows():\n",
    "\n",
    "        if n_pc == 2:\n",
    "            _dist_c0 = sdist.cityblock([transf_point[0], transf_point[1]],\n",
    "                                        [centroids_arr [0, 0], centroids_arr [0, 1]])\n",
    "\n",
    "            _dist_c1 = sdist.cityblock([transf_point[0], transf_point[1]],\n",
    "                                        [centroids_arr [1, 0], centroids_arr [1, 1]])\n",
    "\n",
    "            _dist_c2 = sdist.cityblock([transf_point[0], transf_point[1]],\n",
    "                                        [centroids_arr [2, 0], centroids_arr [2, 1]])\n",
    "        elif n_pc == 3:\n",
    "            _dist_c0 = sdist.cityblock([transf_point[0], transf_point[1], transf_point[2]],\n",
    "                                        [centroids_arr [0, 0], centroids_arr [0, 1], centroids_arr [0, 2]])\n",
    "\n",
    "            _dist_c1 = sdist.cityblock([transf_point[0], transf_point[1], transf_point[2]],\n",
    "                                        [centroids_arr [1, 0], centroids_arr [1, 1], centroids_arr [1, 2]])\n",
    "\n",
    "            _dist_c2 = sdist.cityblock([transf_point[0], transf_point[1], transf_point[2]],\n",
    "                                        [centroids_arr [2, 0], centroids_arr [2, 1], centroids_arr [2, 2]])\n",
    "\n",
    "        distances = np.array([_dist_c0, _dist_c1, _dist_c2])\n",
    "\n",
    "        calculated_classe = np.where(distances == np.amin(distances))[0][0]\n",
    "        actual_classe = transf_point.loc['classe']\n",
    "\n",
    "        if calculated_classe != actual_classe:\n",
    "            _err = np.array([index, calculated_classe, actual_classe])\n",
    "            _errors_vect = np.concatenate((_errors_vect, _err))\n",
    "            _err_count += 1\n",
    "\n",
    "    _errors_vect = _errors_vect.reshape(_err_count, 3)\n",
    "    _errors_frame = pd.DataFrame(_errors_vect, columns=['index', 'calculated class', 'acutal class'])\n",
    "    return _errors_frame\n",
    "\n",
    "def GetMahalanobisDistancesErrors(transformed_values_labelled, centroids_arr, n_pc):\n",
    "    _errors_vect = np.array([])\n",
    "    _err_count = 0\n",
    "\n",
    "    if n_pc == 2:\n",
    "        # _arr = np.array([transformed_values_labelled.loc[:,'attribut1'],transformed_values_labelled.loc[:,'attribut2']])\n",
    "        # _inv_cov = np.linalg.inv(np.cov(_arr))\n",
    "\n",
    "        _arr = np.array([transf_C0.loc[:,'attribut1'],transf_C0.loc[:,'attribut2']])\n",
    "        _inv_cov_c0 = np.linalg.inv(np.cov(_arr))\n",
    "        _arr = np.array([transf_C1.loc[:,'attribut1'],transf_C1.loc[:,'attribut2']])\n",
    "        _inv_cov_c1 = np.linalg.inv(np.cov(_arr))\n",
    "        _arr = np.array([transf_C2.loc[:,'attribut1'],transf_C2.loc[:,'attribut2']])\n",
    "        _inv_cov_c2 = np.linalg.inv(np.cov(_arr))\n",
    "\n",
    "    elif n_pc ==3:\n",
    "        # _arr = np.array([transformed_values_labelled.loc[:,'attribut1'],transformed_values_labelled.loc[:,'attribut2'],transformed_values_labelled.loc[:,'attribut3']])\n",
    "        # _inv_cov = np.linalg.inv(np.cov(_arr))\n",
    "\n",
    "        _arr = np.array([transf_C0.loc[:,'attribut1'],transf_C0.loc[:,'attribut2'],transf_C0.loc[:,'attribut3']])\n",
    "        _inv_cov_c0 = np.linalg.inv(np.cov(_arr))\n",
    "        _arr = np.array([transf_C1.loc[:,'attribut1'],transf_C1.loc[:,'attribut2'],transf_C1.loc[:,'attribut3']])\n",
    "        _inv_cov_c1 = np.linalg.inv(np.cov(_arr))\n",
    "        _arr = np.array([transf_C2.loc[:,'attribut1'],transf_C2.loc[:,'attribut2'],transf_C2.loc[:,'attribut3']])\n",
    "        _inv_cov_c2 = np.linalg.inv(np.cov(_arr))\n",
    "\n",
    "    for index, transf_point in transformed_values_labelled[transformed_values_labelled.classe == 0].iterrows():\n",
    "        if n_pc == 2:\n",
    "            _dist_c0 = sdist.mahalanobis([transf_point[0], transf_point[1]],\n",
    "                                        [centroids_arr [0, 0], centroids_arr [0, 1]], _inv_cov_c0)\n",
    "        elif n_pc == 3:\n",
    "            _dist_c0 = sdist.mahalanobis([transf_point[0], transf_point[1], transf_point[2]],\n",
    "                                        [centroids_arr [0, 0], centroids_arr [0, 1], centroids_arr [0, 2]], _inv_cov_c0)\n",
    "\n",
    "    for index, transf_point in transformed_values_labelled[transformed_values_labelled.classe == 1].iterrows():\n",
    "        if n_pc == 2:          \n",
    "            _dist_c1 = sdist.mahalanobis([transf_point[0], transf_point[1]],\n",
    "                                        [centroids_arr [1, 0], centroids_arr [1, 1]], _inv_cov_c1)\n",
    "\n",
    "        elif n_pc == 3:        \n",
    "            _dist_c1 = sdist.mahalanobis([transf_point[0], transf_point[1], transf_point[2]],\n",
    "                                        [centroids_arr [1, 0], centroids_arr [1, 1], centroids_arr [1, 2]], _inv_cov_c1)\n",
    "\n",
    "    for index, transf_point in transformed_values_labelled[transformed_values_labelled.classe == 2].iterrows():\n",
    "        if n_pc == 2:\n",
    "            _dist_c2 = sdist.mahalanobis([transf_point[0], transf_point[1]],\n",
    "                                        [centroids_arr [2, 0], centroids_arr [2, 1]], _inv_cov_c2)\n",
    "        elif n_pc == 3:\n",
    "            _dist_c2 = sdist.mahalanobis([transf_point[0], transf_point[1], transf_point[2]],\n",
    "                                        [centroids_arr [2, 0], centroids_arr [2, 1], centroids_arr [2, 2]], _inv_cov_c2)\n",
    "\n",
    "        distances = np.array([_dist_c0, _dist_c1, _dist_c2])\n",
    "\n",
    "        calculated_classe = np.where(distances == np.amin(distances))[0][0]\n",
    "        actual_classe = transf_point.loc['classe']\n",
    "\n",
    "        if calculated_classe != actual_classe:\n",
    "            _err = np.array([int(index), int(calculated_classe), int(actual_classe)])\n",
    "            _errors_vect = np.concatenate((_errors_vect, _err))\n",
    "            _err_count += 1\n",
    "\n",
    "    _errors_vect = _errors_vect.reshape(_err_count, 3)\n",
    "    _errors_frame = pd.DataFrame(_errors_vect, columns=['index', 'calculated class', 'acutal class'])\n",
    "    return _errors_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Erreur de classification en utilisant la distance euclidienne et 3 composantes principales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "euclid_dist_errors_3PC = GetEuclidianDistancesErrors(transf_centered_all_labeled, centroids, 3)\n",
    "euclid_dist_errors_3PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erreur de classification en utilisant la distance euclidienne et 2 composantes principales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "euclid_dist_errors_2PC = GetEuclidianDistancesErrors(transf_centered_all_labeled, centroids, 2)\n",
    "euclid_dist_errors_2PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erreur de classification en utilisant la distance de manhattan et 3 composantes principales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "manhatan_dist_errors_3PC = GetManhattanDistancesErrors(transf_centered_all_labeled, centroids, 3)\n",
    "manhatan_dist_errors_3PC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erreur de classification en utilisant la distance de manhattan et 2 composantes principales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "manhatan_dist_errors_2PC = GetManhattanDistancesErrors(transf_centered_all_labeled, centroids, 2)\n",
    "manhatan_dist_errors_2PC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erreur de classification en utilisant la distance de mahalanobis et 3 composantes principales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mahalanobis_dist_errors_3PC = GetMahalanobisDistancesErrors(transf_centered_all_labeled, centroids, 3)\n",
    "mahalanobis_dist_errors_3PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Erreur de classification en utilisant la distance de mahalanobis et 2 composantes principales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mahalanobis_dist_errors_2PC = GetMahalanobisDistancesErrors(transf_centered_all_labeled, centroids, 2)\n",
    "mahalanobis_dist_errors_2PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def PlotPC3D(X, Y, Z, elevation, azimut, error_vector):\n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig, elev=elevation, azim=azimut)\n",
    "    ax.scatter(X, Y, Z, c= labels)\n",
    "\n",
    "    # Plots centroids\n",
    "    ax.scatter(centroids_C0[0], centroids_C0[1], centroids_C0[2], marker='x', c='r', depthshade=True)\n",
    "    ax.scatter(centroids_C1[0], centroids_C1[1], centroids_C1[2], marker='x', c='r', depthshade=True)\n",
    "    ax.scatter(centroids_C2[0], centroids_C2[1], centroids_C2[2], marker='x', c='r', depthshade=True)\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.set_zlabel('PC3')\n",
    "\n",
    "    for index, err in error_vector.iterrows():\n",
    "        err_vec_xs = np.array([transf_centered_all[int(err[0]), 0], centroids[int(err[1]), 0]])\n",
    "        err_vec_ys = np.array([transf_centered_all[int(err[0]), 1], centroids[int(err[1]), 1]])\n",
    "        err_vec_zs = np.array([transf_centered_all[int(err[0]), 2], centroids[int(err[1]), 2]])\n",
    "        ax.plot(err_vec_xs, err_vec_ys, err_vec_zs, 'r--')\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "_, ax1 = PlotPC3D(transf_centered_all[:, 0], transf_centered_all[:, 1], transf_centered_all[:, 2], 45, 45, euclid_dist_errors_3PC)\n",
    "_, ax2 = PlotPC3D(transf_centered_all[:, 0], transf_centered_all[:, 1], transf_centered_all[:, 2], 90, 270, euclid_dist_errors_3PC)\n",
    "_, ax3 = PlotPC3D(transf_centered_all[:, 0], transf_centered_all[:, 1], transf_centered_all[:, 2], 0, 0, euclid_dist_errors_3PC)\n",
    "_, ax4 = PlotPC3D(transf_centered_all[:, 0], transf_centered_all[:, 1], transf_centered_all[:, 2], 0, 90, euclid_dist_errors_3PC)\n",
    "\n",
    "ax1.set_title('Erreurs de classification en utilisant la distance euclidienne - vue isométrique')\n",
    "ax2.set_title('Erreurs de classification en utilisant la distance euclidienne - projection sur PC1 & PC2')\n",
    "ax3.set_title('Erreurs de classification en utilisant la distance euclidienne')\n",
    "ax4.set_title('Erreurs de classification en utilisant la distance euclidienne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, ax1 = PlotPC3D(transf_centered_all[:, 0], transf_centered_all[:, 1], transf_centered_all[:, 2], 45, 45, mahalanobis_dist_errors_2PC)\n",
    "_, ax2 = PlotPC3D(transf_centered_all[:, 0], transf_centered_all[:, 1], transf_centered_all[:, 2], 90, 270, mahalanobis_dist_errors_2PC)\n",
    "_, ax3 = PlotPC3D(transf_centered_all[:, 0], transf_centered_all[:, 1], transf_centered_all[:, 2], 0, 0, mahalanobis_dist_errors_2PC)\n",
    "_, ax4 = PlotPC3D(transf_centered_all[:, 0], transf_centered_all[:, 1], transf_centered_all[:, 2], 0, 90, mahalanobis_dist_errors_2PC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Q3\n",
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_iter = 100\n",
    "cumul_error = 0\n",
    "\n",
    "for test_iter in range(n_iter):\n",
    "    #Prepare data\n",
    "    train_set, test_set = train_test_split(transf_centered_all_labeled, train_size=0.8)\n",
    "    # Train KNN model\n",
    "    _inv = np.linalg.inv(np.cov(np.array([train_set.loc[:,'attribut1'], train_set.loc[:,'attribut2']])))\n",
    "    neigh_classifier = KNeighborsClassifier(n_neighbors=5, metric = 'mahalanobis', metric_params ={\"VI\":_inv})\n",
    "    neigh_classifier.fit(train_set.loc[:,'attribut1':'attribut2'], train_set.loc[:, 'classe'])\n",
    "    # Predict using model\n",
    "    neigh_prediction = neigh_classifier.predict(test_set.loc[:,'attribut1':'attribut2'])\n",
    "    # Calculate errors on predictions\n",
    "    neigh_predict_errors = np.abs(neigh_prediction - np.array(test_set.loc[:,'classe'] !=0))\n",
    "    neigh_predict_errors = np.sum(neigh_predict_errors)\n",
    "    cumul_error += neigh_predict_errors\n",
    "\n",
    "average_neigh_error = cumul_error/n_iter\n",
    "md('L\\'erreur moyenne de prediction pour le KNN utilisant la distance de mahalanobis sur {0} itérations est de {1:3.1%}'.format(n_iter, average_neigh_error/12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cumul_error = 0\n",
    "\n",
    "for test_iter in range(n_iter):\n",
    "    #Prepare data\n",
    "    train_set, test_set = train_test_split(transf_centered_all_labeled, train_size=0.8)\n",
    "    # Train KNN model\n",
    "    _inv = np.linalg.inv(np.cov(np.array([train_set.loc[:,'attribut1'], train_set.loc[:,'attribut2']])))\n",
    "    centroid_classifier = NearestCentroid(metric = 'mahalanobis')\n",
    "    centroid_classifier.fit(train_set.loc[:,'attribut1':'attribut2'], train_set.loc[:, 'classe'])\n",
    "    # Predict using model\n",
    "    centroid_prediction = centroid_classifier.predict(test_set.loc[:,'attribut1':'attribut2'])\n",
    "    # Calculate errors on predictions\n",
    "    centroid_predict_errors = np.abs(centroid_prediction - np.array(test_set.loc[:,'classe'] !=0))\n",
    "    centroid_predict_errors = np.sum(centroid_predict_errors)\n",
    "    cumul_error += centroid_predict_errors\n",
    "\n",
    "average_centroid_error = cumul_error/n_iter\n",
    "\n",
    "md('L\\'erreur moyenne de prediction pour le nearest centroid utilisant la distance de mahalanobis sur {0} itérations est de {1:3.1%}'.format(n_iter, average_centroid_error/12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3\n",
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The next section has been adapted from the code available on the Sci-Kit learn web site:\n",
    "https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py\n",
    "\n",
    "Start of adapted section\n",
    "\"\"\"\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "def make_ellipses(gmm, ax):\n",
    "    for n, color in enumerate(colors):\n",
    "        if gmm.covariance_type == 'full':\n",
    "            covariances = gmm.covariances_[n][:2, :2]\n",
    "        elif gmm.covariance_type == 'tied':\n",
    "            covariances = gmm.covariances_[:2, :2]\n",
    "        elif gmm.covariance_type == 'diag':\n",
    "            covariances = np.diag(gmm.covariances_[n][:2])\n",
    "        elif gmm.covariance_type == 'spherical':\n",
    "            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n",
    "        v, w = np.linalg.eigh(covariances)\n",
    "        u = w[0] / np.linalg.norm(w[0])\n",
    "        angle = np.arctan2(u[1], u[0])\n",
    "        angle = 180 * angle / np.pi  # convert to degrees\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],\n",
    "                                  180 + angle, color=color)\n",
    "        ell.set_clip_box(ax.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)\n",
    "        ax.set_aspect('equal', 'datalim')\n",
    "\n",
    "# Break up the dataset into non-overlapping training (75%) and testing\n",
    "# (25%) sets.\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "# Only take the first fold.\n",
    "train_index, test_index = next(iter(skf.split(transf_centered_all_labeled.loc[:,'attribut1':'attribut2'], transf_centered_all_labeled.loc[:,'classe'])))\n",
    "\n",
    "features_train = transf_centered_all_labeled.loc[train_index,'attribut1':'attribut2']\n",
    "labels_train = transf_centered_all_labeled.loc[train_index, 'classe']\n",
    "features_test = transf_centered_all_labeled.loc[test_index,'attribut1':'attribut2']\n",
    "labels_test = transf_centered_all_labeled.loc[test_index, 'classe']\n",
    "\n",
    "n_classes = len(np.unique(labels_train))\n",
    "\n",
    "# Try GMMs using different types of covariances.\n",
    "estimators = {cov_type: GaussianMixture(n_components=n_classes,\n",
    "              covariance_type=cov_type, max_iter=20, random_state=0)\n",
    "              for cov_type in ['spherical', 'diag', 'full', 'tied']}\n",
    "\n",
    "n_estimators = len(estimators)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n",
    "                    left=.01, right=.99)\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(estimators.items()):\n",
    "    # Since we have class labels for the training data, we can\n",
    "    # initialize the GMM parameters in a supervised manner.\n",
    "    estimator.means_init = np.array([features_train[labels_train == i].mean(axis=0)\n",
    "                                    for i in range(n_classes)])\n",
    "\n",
    "    # Train the other parameters using the EM algorithm.\n",
    "    estimator.fit(features_train)\n",
    "\n",
    "    h = plt.subplot(2, n_estimators // 2, index + 1)\n",
    "    make_ellipses(estimator, h)\n",
    "\n",
    "    for n, color in enumerate(colors):\n",
    "        condition = transf_centered_all_labeled['classe'] == n\n",
    "        data = transf_centered_all_labeled.loc[condition , 'attribut1':'attribut2']\n",
    "        plt.scatter(data.loc[:, 'attribut1'], data.loc[:, 'attribut2'], s=0.8, color=color,\n",
    "                    label=[0, 1, 2])\n",
    "    # Plot the test data with crosses\n",
    "    for n, color in enumerate(colors):\n",
    "        data = features_test[labels_test == n]\n",
    "        plt.scatter(data.loc[:, 'attribut1'], data.loc[:, 'attribut2'], marker='x', color=color)\n",
    "\n",
    "    labels_train_pred = estimator.predict(features_train)\n",
    "    train_accuracy = np.mean(labels_train_pred.ravel() == labels_train.ravel()) * 100\n",
    "    plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy,\n",
    "             transform=h.transAxes)\n",
    "\n",
    "    labels_test_pred = estimator.predict(features_test)\n",
    "    test_accuracy = np.mean(labels_test_pred.ravel() == labels_test.ravel()) * 100\n",
    "    plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy,\n",
    "             transform=h.transAxes)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(name)\n",
    "\n",
    "plt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "End of adapted section\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " \n",
    "Prédiction pour l'observation [52.1, 23.0, 6.1, 16.5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obervation = np.array[52.1, 23.0, 6.1, 16.5]\n",
    "estimators['tied'].predict(obervation)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (IFT870-TP1)",
   "language": "python",
   "name": "pycharm-3359ad28"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}